## Attention (wip)

This repository will house a visualization that will attempt to convey instant enlightenment of how <a href="https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/">Attention</a> works, in the field of artificial intelligence.

## What has Attention accomplished?

- [Protein Folding](https://www.nature.com/articles/s41586-021-03819-2)
- [Language](https://arxiv.org/abs/2005.14165)
- [Vision](https://arxiv.org/abs/2010.11929)
- [Image Segmentation](https://arxiv.org/abs/2005.12872)
- [Speech Recognition](https://arxiv.org/abs/2203.15095)
- [Symbolic Mathematics](https://arxiv.org/abs/1912.01412)
- [Music Generation](https://openai.com/blog/musenet/)
- [Theorem Proving](https://arxiv.org/abs/2009.03393)
- [Gene Expression](https://www.nature.com/articles/s41592-021-01252-x)
- [Text to Image](https://openai.com/blog/dall-e/)
- [Text to Video](https://arxiv.org/abs/2111.12417)
- [Code Generation](https://www.deepmind.com/blog/competitive-programming-with-alphacode)
- [Language+](https://arxiv.org/abs/2204.02311)

Will keep adding to this list as time goes on

## Other resources

- [Yannic Kilcher](https://www.youtube.com/watch?v=iDulhoQ2pro)
- [Peter Bloem](http://peterbloem.nl/blog/transformers)
- [Jay Alammar](http://jalammar.github.io/illustrated-transformer/)
- [Sasha Rush](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

## Appreciation

Large thanks goes to <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">3Blue1Brown</a> for showing us that complex mathematics can be taught with such elegance and potency through visualizations

## Citations

```bibtex
@misc{vaswani2017attention,
    title   = {Attention Is All You Need},
    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year    = {2017},
    eprint  = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}
```
